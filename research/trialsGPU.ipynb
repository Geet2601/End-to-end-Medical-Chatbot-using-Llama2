{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "print(\"OK!\")\n",
    "\n",
    "# %%\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "import pinecone\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import CTransformers\n",
    "import os\n",
    "import torch\n",
    "\n",
    "os.chdir('c:/Users/gt260/End-to-end-Medical-Chatbot-using-Llama2')\n",
    "print(os.getcwd())\n",
    "\n",
    "# %%\n",
    "PINECONE_API_KEY = \"122f095f-ea22-497c-99a0-2994b23f687d\"\n",
    "# PINECONE_API_ENV = \"quickstart\"\n",
    "\n",
    "\n",
    "# from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# %%\n",
    "#Extract data from the PDF\n",
    "def load_pdf(data):\n",
    "    loader = DirectoryLoader(data,\n",
    "                    glob=\"*.pdf\",\n",
    "                    loader_cls=PyPDFLoader)\n",
    "    \n",
    "    documents = loader.load()\n",
    "\n",
    "    return documents\n",
    "\n",
    "# %%\n",
    "extracted_data = load_pdf(\"data/\")\n",
    "\n",
    "# %%\n",
    "# extracted_data\n",
    "\n",
    "# %%\n",
    "#Create text chunks\n",
    "def text_split(extracted_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 20)\n",
    "    text_chunks = text_splitter.split_documents(extracted_data)\n",
    "\n",
    "    return text_chunks\n",
    "\n",
    "# %%\n",
    "text_chunks = text_split(extracted_data)\n",
    "print(\"length of my chunk:\", len(text_chunks))\n",
    "\n",
    "# %%\n",
    "# text_chunks\n",
    "\n",
    "# %%\n",
    "#download embedding model\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embeddings\n",
    "\n",
    "# %%\n",
    "embeddings = download_hugging_face_embeddings();\n",
    "\n",
    "# %%\n",
    "# embeddings\n",
    "\n",
    "# %%\n",
    "query_result = embeddings.embed_query(\"Hello world\")\n",
    "print(\"Length\", len(query_result))\n",
    "\n",
    "# %%\n",
    "query_result\n",
    "\n",
    "# %%\n",
    "os.environ['PINECONE_API_KEY'] = PINECONE_API_KEY\n",
    "index_name = \"medical-chatbot\"\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# %%\n",
    "\n",
    "# texts = [\"Tonight, I call on the Senate to: Pass the Freedom to Vote Act.\", \"ne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.\", \"One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.\"]\n",
    "\n",
    "# docsearch = PineconeVectorStore.from_texts(texts, embedding = embeddings, index_name=index_name)\n",
    "# docsearch.add_texts([t.page_content for t in text_chunks])\n",
    "\n",
    "# %%\n",
    "#If we already have an index we can load it like this\n",
    "docsearch = PineconeVectorStore.from_existing_index(index_name, embeddings)\n",
    "query  = \"Why am i feeling sick in a dusty room?\"\n",
    "docs = docsearch.similarity_search(query, k=3)\n",
    "print(\"Result\", docs)\n",
    "\n",
    "# %%\n",
    "prompt_template=\"\"\"\n",
    "Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "PROMPT=PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "chain_type_kwargs={\"prompt\": PROMPT}\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "# %%\n",
    "llm=CTransformers(model=\"model/llama-2-7b-chat.ggmlv3.q4_0.bin\",\n",
    "                  model_type=\"llama\",\n",
    "                  config={'max_new_tokens':512,\n",
    "                          'temperature':0.8})\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "# %%\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=docsearch.as_retriever(search_kwargs={'k': 2}),\n",
    "    return_source_documents=True, \n",
    "    chain_type_kwargs=chain_type_kwargs\n",
    "    )\n",
    "\n",
    "# %%\n",
    "while True:\n",
    "    user_input=input(f\"Input Prompt:\")\n",
    "    result=qa({\"query\": user_input})\n",
    "    print(\"Response : \", result[\"result\"])\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mchatbot02",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
