{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "import pinecone\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import CTransformers\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-08 12:36:07.286 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\Users\\gt260\\anaconda3\\envs\\mchatbot02\\lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2024-07-08 12:36:07.287 \n",
      "`st.cache` is deprecated and will be removed soon. Please use one of Streamlit's new caching commands, `st.cache_data` or `st.cache_resource`.\n",
      "More information [in our docs](https://docs.streamlit.io/develop/concepts/architecture/caching).\n",
      "\n",
      "**Note**: The behavior of `st.cache` was updated in Streamlit 1.36 to the new caching logic used by `st.cache_data` and `st.cache_resource`.\n",
      "This might lead to some problems or unexpected behavior in certain edge cases.\n",
      "\n",
      "2024-07-08 12:36:07.288 \n",
      "`st.cache` is deprecated and will be removed soon. Please use one of Streamlit's new caching commands, `st.cache_data` or `st.cache_resource`.\n",
      "More information [in our docs](https://docs.streamlit.io/develop/concepts/architecture/caching).\n",
      "\n",
      "**Note**: The behavior of `st.cache` was updated in Streamlit 1.36 to the new caching logic used by `st.cache_data` and `st.cache_resource`.\n",
      "This might lead to some problems or unexpected behavior in certain edge cases.\n",
      "\n",
      "2024-07-08 12:36:07.289 \n",
      "`st.cache` is deprecated and will be removed soon. Please use one of Streamlit's new caching commands, `st.cache_data` or `st.cache_resource`.\n",
      "More information [in our docs](https://docs.streamlit.io/develop/concepts/architecture/caching).\n",
      "\n",
      "**Note**: The behavior of `st.cache` was updated in Streamlit 1.36 to the new caching logic used by `st.cache_data` and `st.cache_resource`.\n",
      "This might lead to some problems or unexpected behavior in certain edge cases.\n",
      "\n",
      "2024-07-08 12:36:07.290 Session state does not function when running a script without `streamlit run`\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Change working directory\n",
    "os.chdir('c:/Users/gt260/End-to-end-Medical-Chatbot-using-Llama2')\n",
    "\n",
    "# Initialize Pinecone\n",
    "PINECONE_API_KEY = \"122f095f-ea22-497c-99a0-2994b23f687d\"\n",
    "index_name = \"medical-chatbot\"\n",
    "os.environ['PINECONE_API_KEY'] = PINECONE_API_KEY\n",
    "\n",
    "# Functions to load data and create embeddings\n",
    "@st.cache(allow_output_mutation=True)\n",
    "def load_pdf(data):\n",
    "    loader = DirectoryLoader(data, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "@st.cache(allow_output_mutation=True)\n",
    "def download_hugging_face_embeddings():\n",
    "    return HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "@st.cache(allow_output_mutation=True)\n",
    "def initialize_pinecone():\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    index = pc.Index(index_name)\n",
    "    return index\n",
    "\n",
    "# def text_split(extracted_data):\n",
    "#     text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "#     text_chunks = text_splitter.split_documents(extracted_data)\n",
    "#     return text_chunks\n",
    "\n",
    "def create_vector_store(text_chunks, embeddings):\n",
    "    docsearch = PineconeVectorStore.from_texts([t.page_content for t in text_chunks], embedding=embeddings, index_name=index_name)\n",
    "    docsearch.add_texts([t.page_content for t in text_chunks])\n",
    "    return docsearch\n",
    "\n",
    "def setup_retrieval_qa(docsearch, embeddings):\n",
    "    prompt_template = \"\"\"\n",
    "    Use the following pieces of information to answer the user's question.\n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "\n",
    "    Only return the helpful answer below and nothing else.\n",
    "    Helpful answer:\n",
    "    \"\"\"\n",
    "    PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "    chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "\n",
    "    llm = CTransformers(model=\"model/llama-2-7b-chat.ggmlv3.q4_0.bin\",\n",
    "                        model_type=\"llama\",\n",
    "                        config={'max_new_tokens': 512, 'temperature': 0.8})\n",
    "\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm=llm, \n",
    "        chain_type=\"stuff\", \n",
    "        retriever=docsearch.as_retriever(search_kwargs={'k': 2}),\n",
    "        return_source_documents=True, \n",
    "        chain_type_kwargs=chain_type_kwargs\n",
    "    )\n",
    "    return qa\n",
    "\n",
    "# Streamlit UI\n",
    "st.title(\"Medical Chatbot using LLaMA 2\")\n",
    "\n",
    "if 'qa' not in st.session_state:\n",
    "    st.session_state['qa'] = None\n",
    "\n",
    "st.sidebar.title(\"Configuration\")\n",
    "st.sidebar.write(\"Upload your PDF files to create the chatbot.\")\n",
    "\n",
    "uploaded_files = st.sidebar.file_uploader(\"Upload PDFs\", accept_multiple_files=True, type=[\"pdf\"])\n",
    "if uploaded_files:\n",
    "    for uploaded_file in uploaded_files:\n",
    "        with open(os.path.join(\"data\", uploaded_file.name), \"wb\") as f:\n",
    "            f.write(uploaded_file.getbuffer())\n",
    "    st.sidebar.success(\"PDFs uploaded successfully!\")\n",
    "\n",
    "    if st.sidebar.button(\"Initialize Chatbot\"):\n",
    "        with st.spinner(\"Loading data and initializing...\"):\n",
    "            extracted_data = load_pdf(\"data/\")\n",
    "            text_chunks = text_split(extracted_data)\n",
    "            embeddings = download_hugging_face_embeddings()\n",
    "            pc = initialize_pinecone()\n",
    "            docsearch = create_vector_store(text_chunks, embeddings)\n",
    "            st.session_state['qa'] = setup_retrieval_qa(docsearch, embeddings)\n",
    "        st.sidebar.success(\"Chatbot initialized successfully!\")\n",
    "\n",
    "if st.session_state['qa']:\n",
    "    st.write(\"Chat with the medical chatbot below:\")\n",
    "    user_input = st.text_input(\"Your question:\", \"\")\n",
    "    if user_input:\n",
    "        result = st.session_state['qa']({\"query\": user_input})\n",
    "        st.write(\"Response: \", result[\"result\"])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mchatbot02",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
